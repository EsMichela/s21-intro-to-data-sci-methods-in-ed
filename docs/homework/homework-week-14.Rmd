---
title: "Homework Week 14 - Network and Text Analysis"
output: html_document 
---

``` {r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

In this homework, we'll carry out a text analysis. For reference, the slides from class are here:

https://making-data-science-count.github.io/s21-intro-to-data-sci-methods-in-ed/slides/presentations-week-14.html#1

A relevant resource for the tidytext package is here

https://www.tidytextmining.com

We'll focus on term frequencies, though you'll have a chance to carry out a sentiment analysis, too, for a reach.

We'll be using a data frame representing responses from students to the question, 
"What major factors led you into teaching?".

## Term frequencies

First, download the file qual-data.csv from the repository. It is available
here: https://github.com/making-data-science-count/s21-intro-to-data-sci-methods-in-ed/tree/main/data/qual-data.csv. Save it to the directory you're using for this project and
read this file in to R using the read_csv() function loaded from the tidyverse in the next code chunk.

```{r}
d <- read_csv("../data/qual-data.csv")

d
```

Take a look at the responses; what do you notice? Keep these things in mind as you interpret
the output from the methods you use below.

Next, load (and, if you haven't at this point, install) the tidytext package.

Then, use the unnest_tokens() function to "unnest" the terms in the column representing
the text response in the data you loaded. We recommend naming the new column "word",
as this will make a later step more straightforward.

Save the result to a new object (preferably one with a different name than the name
you used when you read in the data).

If you need help, consider the documentation for unnest_tokens here: 
https://juliasilge.github.io/tidytext/reference/unnest_tokens.html

```{r}
library(tidytext)

dd <- d %>% 
  unnest_tokens(word, Response)
```

What terms are most frequent? To answer this question, use the count() function, counting
up the values of the column in your data set representing the terms. You may find the
sort argument for count handy; check out ?count to see how to use it.

```{r}
dd %>% 
  count(word, sort = TRUE)
```

Last, use a list of stopwords to filter the most frequent words; copy the code
you used above, but join (using anti_join) the built-in (to tidytext) data frame of
stopwords, or common words, named stop_words.

```{r}
dd %>% 
  count(word, sort = TRUE) %>% 
  anti_join(stop_words)
```

What do you notice? And, how does what you notice align with and capture what you
noticed from reading the responses earlier - and what does not align or capture
the nature of those responses? This is a chance to reflect on how data science or
computational methods are not necessarily a panacea - they reveal some things, while
overlooking or even obscuring others!

Please add two or more thoughts below:

- 
- 

## Reach 1

For this reach, create a plot of the most frequent words. Tip - you may want to
_filter_ the top words, otherwise the plot may represent too many terms to be
able to be easily interpreted!

```{r}

```

## Reach 2

For this reach, carry out a sentiment analysis. See the instructions here:
https://www.tidytextmining.com/sentiment.html (and check the slides for an example).

```{r}

```

## Self-assessment and reflection

Respond to the following three questions on a 1 (not at all) to 5 (very much)
scale by replacing the "x" below with your response:

``` {r, reflection}
x = NULL
tibble::tribble(
  ~question,                                   ~response,
  "How challenging was this homework?",        x,
  "How interesting was this homework to you?", x,
  "How valuable was this homework to you?",    x
)
```

Include any other comments, feedback, or reflections on this homework below:

Important note: Please post your comment(s), feedback, or reflections in Slack
when you share your Rmd file!
